{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8201fa6c-9a59-4ff5-9c28-ba371f6e462d",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755ffa3c-2c60-4e49-a425-21ca7c991c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch import Generator, Tensor, mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8a85cc-353f-4f88-9d7a-c565b4033fcd",
   "metadata": {},
   "source": [
    "### 4.2 Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0115a2c-3153-4c64-a4b6-a971bec52c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class ImageFolderWithSize(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.samples = datasets.ImageFolder(root=root).samples\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path)\n",
    "        original_size = img.size  # (width, height)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label, original_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021f11f0-6076-487e-89a0-40d46897fe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(dataset, num_classes=3):\n",
    "    \"\"\"Compute class weights for an imbalanced dataset.\"\"\"\n",
    "    counts = [0.0] * num_classes\n",
    "    for _, label in dataset.samples:\n",
    "        counts[label] += 1\n",
    "\n",
    "    mean_count = sum(counts) / num_classes\n",
    "    weights = Tensor([mean_count / c for c in counts])\n",
    "\n",
    "    # not my best work hardcoding the labels here... \n",
    "    # but for this assignments I'll let it slide. \n",
    "    print(f\"Class counts: Normal: {counts[0]}, Benign: {counts[1]}, Malignant: {counts[2]}\")\n",
    "    print(f\"Class weights: {weights.tolist()}\")\n",
    "    return weights\n",
    "\n",
    "\n",
    "def init_dataset(transform, seed=69420, compute_weights=False, compute_size_freq=False):\n",
    "    dataset = ImageFolderWithSize('data/', transform=transform)\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(0.7 * total_size)\n",
    "    val_size = int(0.15 * total_size)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    # Compute weights\n",
    "    weights = None\n",
    "    if compute_weights:\n",
    "        weights = compute_class_weights(dataset)\n",
    "\n",
    "    # Compute image size frequencies\n",
    "    size_freq = None\n",
    "    if compute_size_freq:\n",
    "        size_freq = {}\n",
    "        for _, _, orig_size in dataset:\n",
    "            size_freq[orig_size] = size_freq.get(orig_size, 0) + 1\n",
    "        print(f\"Image size frequency count: {size_freq}\")\n",
    "\n",
    "    # For reproducibility\n",
    "    generator = Generator().manual_seed(seed)\n",
    "\n",
    "    return random_split(\n",
    "        dataset, [train_size, val_size, test_size], generator=generator\n",
    "    ), weights, size_freq\n",
    "\n",
    "# batch_size suitable for a GTX 1650ti\n",
    "def load_dataset(dataset, batch_size=16, pin=True):\n",
    "    train, val, test = dataset\n",
    "    return (\n",
    "        DataLoader(train, batch_size=batch_size, shuffle=True, pin_memory=pin),\n",
    "        DataLoader(val, batch_size=batch_size, shuffle=False, pin_memory=pin),\n",
    "        DataLoader(test, batch_size=batch_size, shuffle=False, pin_memory=pin)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9e4945-cbe6-4f5c-b9f9-adb9e9d7b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Resize, Grayscale, Normalize, ToTensor\n",
    "cnn_dataset, weights, freqs = init_dataset(Compose([\n",
    "    Resize((256, 256)),\n",
    "    Grayscale(num_output_channels=1),\n",
    "    ToTensor()\n",
    "]), compute_weights=True, compute_size_freq=True)\n",
    "\n",
    "cnn_dataset = load_dataset(cnn_dataset)\n",
    "\n",
    "vgg_dataset, _, _ = init_dataset(Compose([\n",
    "    Resize((224,224)),\n",
    "    ToTensor(),\n",
    "    Grayscale(num_output_channels=3),\n",
    "    Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "]))\n",
    "vgg_dataset = load_dataset(vgg_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbfb5b5-2ca9-4c89-abea-30022776012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def show_sample(loader, n=3, force_channels=None, title=None):\n",
    "    \"\"\"\n",
    "    Display 'n' samples from a dataloader using matplotlib subplots.\n",
    "\n",
    "    force_channels:\n",
    "        None  -> infer from tensor (use 1 channel = grayscale, 3 = RGB)\n",
    "        1     -> convert to grayscale before display\n",
    "        3     -> convert to RGB before display\n",
    "\n",
    "    title:\n",
    "        Optional figure-level title\n",
    "    \"\"\"\n",
    "    images, labels, _ = next(iter(loader))  # dimensions: list/tuple of (x, y)\n",
    "    print\n",
    "\n",
    "    n = min(n, images.shape[0])\n",
    "    fig, axes = plt.subplots(1, n, figsize=(4 * n, 4))\n",
    "\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    if title is not None:\n",
    "        fig.suptitle(title, fontsize=14)\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        img = images[i]\n",
    "\n",
    "        # Decide how many channels we want to DISPLAY\n",
    "        if force_channels is not None:\n",
    "            c = force_channels\n",
    "        else:\n",
    "            c = img.shape[0]  # 1 or 3\n",
    "\n",
    "        # Convert tensor to numpy\n",
    "        arr = img.detach().cpu().numpy()\n",
    "\n",
    "        # --- Handle channel variations ---\n",
    "        if c == 1:\n",
    "            # RGB -> grayscale if needed\n",
    "            if img.shape[0] == 3:\n",
    "                arr = arr.mean(axis=0)\n",
    "            else:\n",
    "                arr = arr[0]\n",
    "\n",
    "            ax.imshow(arr, cmap=\"gray\")\n",
    "        \n",
    "        elif c == 3:\n",
    "            # Grayscale -> RGB if needed\n",
    "            if img.shape[0] == 1:\n",
    "                arr = np.repeat(arr, 3, axis=0)\n",
    "\n",
    "            arr = arr.transpose(1, 2, 0)\n",
    "            ax.imshow(arr)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"force_channels must be None, 1, or 3\")\n",
    "\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        # Print dimensions below image\n",
    "        #dim = dimensions[i]\n",
    "        #ax.set_xlabel(f\"Dimensions: {dim}\", fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac1f2de-2903-43f9-956c-adcc1ddda868",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample(cnn_dataset[0])\n",
    "show_sample(vgg_dataset[0]) # will warn because of normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff37e4e5-6d25-4fdf-b9ba-f200e056fe34",
   "metadata": {},
   "source": [
    "### 4.3 Architecture selection and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd565c11-d5f6-4379-8039-381f1c8f556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleLCC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2), # shrink to 128x128\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2), # to 64x64,\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2), # 32x32\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2), # 16x16,\n",
    "            # Added to reduce overfitting\n",
    "            nn.AdaptiveAvgPool2d((8,8)) \n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 8 * 8, 64),\n",
    "            nn.ReLU(),\n",
    "            # Added to reduce overfitting\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(64, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x) \n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af94661-a5e3-49f8-9c01-75658ef9424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "\n",
    "class VGGLCC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        from torchvision.models import vgg16\n",
    "        self.model = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "        # freeze all CNN weights\n",
    "        for p in self.model.features.parameters():\n",
    "            p.requires_grad = False\n",
    "        # replace output layer to 3 labels, these will have requires_grad=True\n",
    "        self.model.classifier[-1] = nn.Linear(self.model.classifier[-1].in_features, 3)\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84fd001-27cb-48c4-9be3-27e1e3f968c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple = SimpleLCC()\n",
    "vgg = VGGLCC()\n",
    "print(simple)\n",
    "print(\"Parameter count:\", sum((p.numel() for p in simple.parameters())))\n",
    "print(vgg)\n",
    "print(\"Parameter count:\", sum((p.numel() for p in vgg.parameters())))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a092a3c6-7ac4-44ca-abbb-43a88d52f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "    \n",
    "def train(device, model, optimizer, loss_fn, epochs, train_dataloader, validation_dataloader, patience=2):\n",
    "    writer = SummaryWriter() # TensorBoard writer\n",
    "    \n",
    "    best_validation_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train Step\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            # Inference\n",
    "            inputs, labels, _image_dims = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Optimization\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_train_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Log training loss and accuracy every 16 batches to TensorBoard\n",
    "            if batch_idx % 16 == 0:\n",
    "                writer.add_scalar('Training Loss/batch', loss.item(), epoch * len(train_dataloader) + batch_idx)\n",
    "                writer.add_scalar('Training Accuracy/batch', (correct_train / total_train), epoch * len(train_dataloader) + batch_idx)\n",
    "                print(f\"Epoch: {epoch}, Batch: {batch_idx}, Training Loss: {loss.item():.4f}, Training Accuracy: {(correct_train / total_train):.4f}\")\n",
    "                writer.flush() # Flush data\n",
    "        \n",
    "        \n",
    "        # Calculate and print average training loss and accuracy for the epoch\n",
    "        avg_train_loss = running_train_loss / len(train_dataloader)\n",
    "        train_accuracy = correct_train / total_train\n",
    "        writer.add_scalar('Training Loss/epoch', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Training Accuracy/epoch', train_accuracy, epoch)\n",
    "        print(f\"Epoch: {epoch}, Average Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "        writer.flush() # Flush data after epoch\n",
    "        \n",
    "        \n",
    "        # Validation Step\n",
    "        model.eval()\n",
    "        running_validation_loss = 0.0\n",
    "        correct_validation = 0\n",
    "        total_validation = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(validation_dataloader):\n",
    "                # Inference\n",
    "                inputs, labels, _image_dims = batch\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                running_validation_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_validation += labels.size(0)\n",
    "                correct_validation += (predicted == labels).sum().item()\n",
    "                \n",
    "                # Log validation loss and accuracy every 16 batches to TensorBoard\n",
    "                if batch_idx % 16 == 0:\n",
    "                    writer.add_scalar('Validation Loss/batch', loss.item(), epoch * len(validation_dataloader) + batch_idx)\n",
    "                    writer.add_scalar('Validation Accuracy/batch', (correct_validation / total_validation), epoch * len(validation_dataloader) + batch_idx)\n",
    "                    print(f\"Epoch: {epoch}, Batch: {batch_idx}, Validation Loss: {loss.item():.4f}, Validation Accuracy: {(correct_validation / total_validation):.4f}\")\n",
    "                    writer.flush() # Flush data\n",
    "        \n",
    "        \n",
    "        # Calculate and print average validation loss and accuracy for the epoch\n",
    "        avg_validation_loss = running_validation_loss / len(validation_dataloader)\n",
    "        validation_accuracy = correct_validation / total_validation\n",
    "        writer.add_scalar('Validation Loss/epoch', avg_validation_loss, epoch)\n",
    "        writer.add_scalar('Validation Accuracy/epoch', validation_accuracy, epoch)\n",
    "        print(f\"Epoch: {epoch}, Average Validation Loss: {avg_validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.4f}\")\n",
    "        writer.flush() # Flush data after epoch\n",
    "        \n",
    "        \n",
    "        # Early stopping logic\n",
    "        \n",
    "        if torch.isnan(inputs).any():\n",
    "            print(\"Input contains NaN!\")\n",
    "            break\n",
    "        \n",
    "        if torch.isnan(outputs).any():\n",
    "            print(\"Model output contains NaN!\")\n",
    "            break\n",
    "            \n",
    "        if torch.isnan(loss):\n",
    "            print(\"Loss is NaN!\")\n",
    "            break\n",
    "        \n",
    "        threshold = 0.05\n",
    "        if avg_validation_loss + threshold < best_validation_loss: \n",
    "            best_validation_loss = avg_validation_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(f\"Early stopping triggered after {patience} epochs with no improvement.\")\n",
    "                break\n",
    "    \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f04cb6-b2d4-4a7e-b9f7-c07204093531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam, SGD \n",
    "gpu = torch.device(\"cuda\")\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b50966-774a-4b45-a72d-a076b8151029",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(simple.parameters(), lr=8e-4)\n",
    "loss = nn.CrossEntropyLoss(weight=weights.to(gpu))\n",
    "train(\n",
    "    gpu, \n",
    "    simple,  \n",
    "    optimizer, \n",
    "    loss, \n",
    "    epochs,\n",
    "    cnn_dataset[0], # training \n",
    "    cnn_dataset[1], # validation\n",
    "    patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643813fd-7a0d-48dc-80c6-667b4df5c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(vgg.model.classifier[-1].parameters(), lr=3e-4)\n",
    "loss = nn.CrossEntropyLoss(weight=weights.to(gpu))\n",
    "train(\n",
    "    gpu, \n",
    "    vgg,  \n",
    "    optimizer, \n",
    "    loss, \n",
    "    epochs,\n",
    "    vgg_dataset[0], # training \n",
    "    vgg_dataset[1], # validation\n",
    "    patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa7c117-144f-425f-9bdf-840d84bd6946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import os\n",
    "import glob\n",
    "\n",
    "log_dir = 'runs/'\n",
    "\n",
    "# Check if the log directory exists\n",
    "if not os.path.exists(log_dir):\n",
    "    print(f\"Error: Log directory '{log_dir}' not found.\")\n",
    "else:\n",
    "    # Find the latest run directory\n",
    "    run_dirs = glob.glob(os.path.join(log_dir, '*'))\n",
    "    if not run_dirs:\n",
    "        print(f\"Error: No run directories found in '{log_dir}'.\")\n",
    "    else:\n",
    "        # Assuming the latest run directory is the one with the latest modification time\n",
    "        latest_run_dir = max(run_dirs, key=os.path.getmtime)\n",
    "        print(f\"Using latest run directory: {latest_run_dir}\")\n",
    "\n",
    "        try:\n",
    "            # Load the TensorBoard event file from the latest run directory\n",
    "            event_acc = EventAccumulator(latest_run_dir)\n",
    "            event_acc.Reload()\n",
    "\n",
    "            # Print available scalar keys\n",
    "            print(\"Available scalar keys:\", event_acc.Tags()['scalars'])\n",
    "\n",
    "            # Extract the scalar data\n",
    "            training_loss_epochs = event_acc.Scalars('Training Loss/epoch')\n",
    "            validation_loss_epochs = event_acc.Scalars('Validation Loss/epoch')\n",
    "\n",
    "            # Get the steps (epochs) and values\n",
    "            epochs_train = [s.step for s in training_loss_epochs]\n",
    "            loss_train = [s.value for s in training_loss_epochs]\n",
    "\n",
    "            epochs_validation = [s.step for s in validation_loss_epochs]\n",
    "            loss_validation = [s.value for s in validation_loss_epochs]\n",
    "\n",
    "\n",
    "            # Plot the losses\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(epochs_train, loss_train, label='Training Loss')\n",
    "            plt.plot(epochs_validation, loss_validation, label='Validation Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Training vs. Validation Loss per Epoch')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: {e}. Make sure the scalar keys exist in the TensorBoard logs.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515cf561-e901-4783-865c-a9a4ec3ba37f",
   "metadata": {},
   "source": [
    "### 4.4 Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dbf010-e3d1-4bd9-9068-78d4af1652d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_model(device, model, test_loader):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch_id, batch in enumerate(test_loader):\n",
    "        inputs, labels_cpu, _ = batch\n",
    "        inputs, labels = inputs.to(device), labels_cpu.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        # Accumulate\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels_cpu.numpy())\n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "def evaluate_results(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    print(classification_report(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        target_names=[\"Normal\", \"Benign\", \"Malignant\"]\n",
    "    ))\n",
    "\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "        xticklabels=[\"Normal\",\"Benign\",\"Malignant\"],\n",
    "        yticklabels=[\"Normal\",\"Benign\",\"Malignant\"]\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix (Test data)\")\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0474642-7a4e-4365-8fd5-baaaca0e49a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SimpleLCC\n",
    "y_pred, y_true = test_model(gpu, simple, cnn_dataset[2])\n",
    "evaluate_results(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475bda8-f5ef-4992-9041-49824c1373df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG16\n",
    "y_pred, y_true = test_model(gpu, vgg, vgg_dataset[2])\n",
    "evaluate_results(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d61c60-d5d1-45db-b2c7-a817b09f462c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
